# master_career_data.yaml
# --------------------------------------------------
# This is the structured, machine-readable full career history
# Extracted and combined from resume.md and cv.md for completeness
# Includes all jobs from CV for full history, with highlights merged from both documents
# Use comments for notes on relevance, archiving, etc.
# Chronological order: most recent first

personal:
  name: Sean Luka Girgis
  title: Senior Data Engineer | Capacity & Infrastructure Optimization
  location: Plano, TX, United States  # Inferred from user context
  email: seanlgirgis@gmail.com
  phone: "214-315-2190"
  linkedin: https://www.linkedin.com/in/sean-girgis-43bb1b5/
  github: https://github.com/seanlgirgis
  twitter: https://x.com/SeanLuka22249  # Or X handle
  website: https://seanlgirgis.github.io

summary:
  short: |-
    Senior Data Engineer & Cloud Architect with 20+ years of enterprise experience. Specialized in migrating legacy on-prem pipelines to Serverless AWS Architectures (Glue/Athena). Expert in PySpark, GenAI Agents (Text-to-SQL), and high-scale Capacity Forecasting.
  long: |-
    # Expanded from CV competencies: Results-driven professional with expertise in Data Engineering (Python, SQL, PySpark, ETL), Cloud & Infrastructure (AWS, Hive/Hadoop, Capacity Planning), and High-Performance Compute (C++, Java, Oracle RAC). Proven track record in performance optimization, data migration, and AI-driven forecasting across telecom, finance, and utility sectors.

experience:
  - company: CITI
    role: Senior Capacity & Data Engineer
    location: ""  # Not specified
    start: 2017-11
    end: 2025-12
    highlights:
      - Architected automated ETL pipelines using Python and Pandas to ingest P95 performance telemetry from 6,000+ endpoints (BMC TrueSight/TSCO), replacing manual Trenda processes.
      - Designed and optimized Oracle Database schemas for historical data retention, enabling long-term trend analysis and seasonal risk forecasting.
      - Developed ML-driven forecasting models using Prophet and scikit-learn to predict infrastructure bottlenecks 6 months in advance, improving provisioning accuracy.
      - Integrated disparate data feeds (CSV, Excel, TSCO) into a unified Oracle reporting framework, providing executive dashboards with real-time utilization insights.
      - Identified underutilized patterns through data mining, leading to significant hardware consolidation.
      # Notes: Core recent role — emphasize for data/cloud/ML jobs

  - company: G6 Hospitality LLC
    role: Performance Engineer
    location: ""  # Not specified
    start: 2017-03
    end: 2017-11
    highlights:
      - Managed end-to-end monitoring for Brand.com using Dynatrace AppMon and Synthetics.
      - Led the "FAST" project, data mining user performance metrics to optimize critical money-generating systems.
      - Supported cloud migration to AWS, evaluated mobile monitoring tools, and delivered before/after dashboards.
      - Optimized large-scale DynaTrace installations, resolving complex issues in high-stakes financial services environments.

  - company: HCL / ENTERGY
    role: APM Consultant
    location: ""  # Not specified
    start: 2017-01
    end: 2017-03
    highlights:
      - Supported enterprise-wide CA APM, CEM, and ADA solutions for utility grid systems.

  - company: CA Technologies
    role: Senior Consultant
    location: ""  # Not specified
    start: 2014-03
    end: 2016-08
    highlights:
      - Led enterprise CA APM implementations and upgrades (v9.1 → v10.1) for financial clients, managing 4,000–6,000 agents across multi-cluster environments.
      - Served as CA APM SME, handling daily operations, cluster maintenance, agent/Power Pack rollouts, and performance bottleneck resolution in J2EE/.NET stacks.
      - Designed custom dashboards, alerts, reports, and Perl/Ksh data-extraction scripts to deliver actionable insights for business-critical monitoring.
      - Provided architectural sizing recommendations, Golden Image creation, client training, and technical team leadership for APM deployments.
      - Optimized agent/Enterprise Manager installations.

  - company: Enterprise Iron (TIAA-CREF)
    role: SME for CA APM
    location: ""  # Not specified
    start: 2011-08
    end: 2013-12
    highlights:
      - Served as CA APM SME for TIAA-CREF, managing 50+ Enterprise Managers and ~4,000–6,000 agents.
      - Designed and implemented custom Management Modules and Perl/Ksh data-extraction scripts.
      - Collaborated with IT teams to troubleshoot performance issues in J2EE/WebLogic environments.

  - company: AT&T
    role: Performance Test Engineer
    location: ""  # Not specified
    start: 2010-08
    end: 2011-07
    highlights:
      - Analyzed performance of J2EE telecom web applications to identify optimal loads and resource bottlenecks.
      - Documented key metrics (JDBC connections, threads, memory, CPU, GC) and installed JMX Monitoring.

  - company: Sabre
    role: Senior Systems & Data Migration Engineer
    location: ""  # Not specified
    start: 2008-05
    end: 2010-01
    highlights:
      - Led the data migration of a shopping engine handling 10x the throughput of VISA, refactoring 200+ MySQL nodes into a high-performance 6-node Oracle RAC cluster.
      - Optimized core transaction processing using C++ and OCCI, reducing physical hardware footprint by 95% while maintaining sub-second query latency.
      - Built testing framework using CPPUNIT in C++/OCCI/OCI environment.
      # Notes: Strong migration example — highlight for data engineering roles

  - company: Computer Science Corporation (CSC)
    role: Architect/Developer
    location: ""  # Not specified
    start: 2007-10
    end: 2008-05
    highlights:
      - Performed UML-based unit design and developed modules for IRS modernization.
      - Worked on CICS/MQSeries/XML messaging architecture with VC++ and DB2.

  - company: Corpus Inc. (CenturyTel - AT&T)
    role: Developer / Support Engineer (AMDOCS billing)
    location: ""  # Not specified
    start: 2005-05
    end: 2007-10
    highlights:
      - Delivered performance enhancements in billing processes (C/C++/Pro*C/PL/SQL), reducing memory usage 75%.
      - Developed and troubleshot Flexible Bill Formatter, EDI interfaces, and Enabler modules.
      - Automated system administration (WebLogic/WebSphere) with Korn Shell scripts.

  - company: Sprint
    role: High Availability Interfaces
    location: ""  # Not specified
    start: 2001
    end: 2005
    highlights:
      - Developed high-availability multithreaded interfaces (C++/POSIX threads/sockets/IPC).
      - Improved DB performance 10x via PL/SQL optimizations during PRMS maintenance.

  - company: Simplex International - Canada
    role: Developer
    location: Canada
    start: 1999
    end: 2001
    highlights:
      - Developed interfaces to the time and attendance system using VB6 and VC++.
      # Notes: Early career — archive or shorten unless relevant

education:
  - degree: High Diploma in Computer Engineering Technology
    school: Humber College
    location: Canada
    year: ""  # Not dated in docs
    gpa: null

  - degree: Bachelor of Science in Civil Engineering
    school: Zagazig University
    location: Egypt
    year: ""  # Not dated in docs
    gpa: null

certifications:
  # No explicit certs listed in resume/cv.md — add as you obtain/remember
  - name: AWS Certified (Inferred from AWS expertise)
    issuer: Amazon Web Services
    year: null
    expires: null
    notes: Placeholder — confirm if you have Data Analytics or similar

flagship_projects:
  - name: Serverless Data Platform (AWS)
    description: Designed a Serverless Lakehouse using S3, Glue Catalog, and Athena. Built a 'Text-to-SQL' GenAI bot using Claude 3 Sonnet to democratize data access. Fixed 'small file' issues by implementing Snappy Parquet compression.
    technologies: [AWS Glue, Athena, Bedrock (GenAI), S3, PySpark]
    impact: Optimized ETL and data access for enterprise scale
    timeframe: Recent  # Adjust date

  - name: HorizonScale — Modernizing Enterprise Capacity with AI & PySpark
    description: Replaced legacy, manual 'Trenda' processes with a modern, agentic data pipeline capable of handling banking-scale telemetry. Architected parallel generator-based pipeline reducing forecasting cycles by 90%. Built interactive Streamlit dashboard serving real-time capacity insights and 'High Trust' utilization scores.
    technologies: [Python, Prophet, Streamlit, Spark, Multiprocessing]
    impact: 90% reduction in forecasting cycles
    timeframe: Recent  # Adjust date

  # Add more from experience if needed, e.g. "FAST" project or Sabre migration
  - name: FAST Project
    description: Data mining user performance metrics to optimize critical money-generating systems.
    technologies: [Dynatrace, Data Mining]
    impact: Optimized revenue-critical systems
    timeframe: 2017