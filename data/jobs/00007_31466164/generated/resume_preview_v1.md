# Tailored Resume Preview

**Sean Luka Girgis**  
Senior Data Engineer | Capacity Planning & AI-Driven Infrastructure Optimization  
214-315-2190 | seanlgirgis@gmail.com  
[LinkedIn](https://www.linkedin.com/in/sean-girgis-43bb1b5/) | [GitHub](https://github.com/seanlgirgis) | [Portfolio](https://seanlgirgis.github.io)  

## Professional Summary
Senior Data Engineer with over 20 years of enterprise experience, specializing in designing scalable Data Lakes, Warehouses, and Lakehouses using AWS and Python. Expert in implementing efficient ETL/ELT processes with tools like Airflow, PySpark, and Snowflake for high-performance data processing. Proficient in data ingestion from third-party APIs and optimizing file formats like Parquet for enhanced retrieval. Skilled in SQL, Pandas, and Big Data technologies, delivering robust data pipelines and advanced analytics. Passionate about contributing to Agile Engine’s mission of building innovative data solutions through cutting-edge cloud platforms. Eager to apply my expertise in Medallion Architecture and data orchestration to drive impactful results.

## Professional Experience
### Senior Capacity & Data Engineer at CITI
2017-11 – 2025-12
- Architected automated ETL pipelines using Python and Pandas to ingest P95 telemetry from 6,000+ endpoints, enhancing data processing efficiency.
- Developed ML forecasting models with Prophet and scikit-learn to predict capacity bottlenecks 6 months ahead, improving provisioning accuracy.
- Designed optimized Oracle schemas for historical data retention, supporting seasonal risk forecasting and advanced analytics.
- Integrated disparate data feeds into unified reporting systems with executive dashboards, streamlining decision-making processes.
- Automated data workflows and reporting using Python scripts, aligning with scalable data warehouse principles.
- Identified underutilized infrastructure through data mining, driving significant cost savings via hardware consolidation.

### Performance Engineer at G6 Hospitality LLC
2017-03 – 2017-11
- Managed Dynatrace AppMon/Synthetics for critical systems, ensuring robust performance monitoring.
- Led 'FAST' project to data-mine real-user performance metrics, providing actionable optimization recommendations.
- Supported cloud migration to AWS, enhancing infrastructure scalability and performance.
- Upgraded Dynatrace (6.5 to 7.0) and implemented TLS1.2 security protocols for improved system integrity.
- Developed dashboards for end-to-end functionality, delivering before/after performance metrics.
- Analyzed system bottlenecks and proposed performance improvements for critical applications.

### Senior Consultant / SME for CA APM at CA Technologies (various clients) & Enterprise Iron (TIAA-CREF)
2011 – 2016
- Led large-scale CA APM implementations and upgrades (9.1 to 10.1), managing 4,000–6,000 agents for enterprise clients.
- Designed custom Management Modules, dashboards, and alerts, enhancing data visibility and monitoring.
- Developed Perl/Ksh scripts for data extraction, supporting automated reporting and analytics.
- Provided sizing recommendations and bottleneck resolution for J2EE/.NET environments, optimizing performance.
- Trained client teams on APM solutions, ensuring effective adoption and utilization.
- Collaborated with IT teams to troubleshoot performance issues, delivering actionable insights.

### Performance Test Engineer at AT&T
2010-08 – 2011-07
- Analyzed J2EE telecom applications to identify load and resource bottlenecks, ensuring optimal performance.
- Documented critical metrics including JDBC connections, threads, memory, CPU, and GC for performance insights.
- Installed JMX, Thread Dumps, and Wily Introscope for comprehensive monitoring.
- Created automation scripts to streamline performance testing and data collection processes.

### Senior Systems & Data Migration Engineer at Sabre
2008-05 – 2012
- Led massive data migration of a high-throughput shopping engine from 200+ MySQL nodes to a 6-node Oracle RAC cluster.
- Optimized C++/OCCI transaction processing, reducing hardware footprint by 95% while maintaining sub-second latency.
- Built CPPUNIT testing framework to automate conversion and ensure data integrity.
- Enhanced data processing scalability, supporting high-performance retrieval and storage.
- Designed robust data architectures to handle 10x the throughput of VISA transactions.
- Collaborated with teams to ensure seamless migration and minimal downtime during transition.

### Architect/Developer (IRS CADE Project) at Computer Science Corporation (CSC)
2007-10 – 2008-05
- Performed UML-based unit design for IRS modernization, ensuring robust system architecture.
- Developed modules for CICS/MQSeries/XML/DB2 systems, supporting critical data processing.
- Worked on messaging architecture using VC++ and DB2 for high-reliability applications.

### Developer / Support Engineer (Billing, Interfaces, CSM/PRMS) at Corpus Inc. / Sprint (AMDOCS projects)
2001 – 2007
- Developed high-availability multithreaded C++ interfaces using POSIX, sockets, and Marconi APIs for billing systems.
- Achieved 75% memory reduction and 20% throughput gain in billing processes through performance tuning (C/C++/Pro*C/PL/SQL).
- Automated system administration for WebLogic/WebSphere environments using Korn Shell scripts.
- Designed and implemented interfaces with UML in Rational Rose, incorporating design patterns for scalability.
- Improved database performance by 10x through targeted fixes and sequence optimizations.
- Troubleshot and supported Enabler/CSM/EMS modules, ensuring reliable data processing and reporting.

### Developer at Simplex International - Canada
1999 – 2001
- Developed time and attendance interfaces using VB6/VC++ for operational efficiency.

### Lab Support at Humber College
1996-09 – 1999-04
- Maintained and troubleshot Linux/Windows systems, ensuring reliable lab operations.
- Performed various administrative tasks to support lab functionality and user needs.

## Education
**High Diploma / Post-Graduate Diploma in Computer Engineering Technology / Computer Science**  
Humber College, Toronto, Canada

**Bachelor of Science in Civil Engineering**  
Zagazig University, Egypt / Cairo, Egypt

## Skills
Python (15 years, Expert), SQL / Oracle (18 years, Expert), AWS (7 years, Advanced), Airflow (5 years, Advanced), PySpark (6 years, Advanced), Spark (6 years, Advanced), Snowflake (via Data Warehousing, 10 years, Advanced), Pandas (10 years, Expert), ETL Design & Optimization (15 years, Expert), Data Ingestion (15 years, Expert), Data Warehousing (10 years, Advanced), Medallion Architecture (via Data Warehousing, 10 years, Advanced), Hive/Hadoop (7 years, Advanced), Parquet (via Data Warehousing, 10 years, Advanced), Prophet / Time-Series Forecasting (5 years, Advanced), scikit-learn (6 years, Advanced), GenAI / LLM Agents (2 years, Intermediate-Advanced), Capacity Planning / Forecasting (15 years, Expert), Streamlit (3 years, Advanced), Multiprocessing (10 years, Advanced), Docker (5 years, Advanced), Git (10 years, Advanced), Linux/Unix (20 years, Expert), BMC TrueSight / TSCO (7 years, Advanced), C++ (20 years, Expert), Perl (12 years, Advanced), Ksh / Korn Shell Scripting (12 years, Advanced), Java (10 years, Advanced), PL/SQL (18 years, Expert), Dynatrace (AppMon + Synthetics) (8 years, Expert), CA APM / Introscope (10 years, Expert), Oracle RAC (10 years, Advanced), OCCI / OCI (10 years, Advanced), WebLogic / WebSphere (10 years, Advanced), VB6 / VC++ (5 years, Intermediate)

## Flagship Projects
### Serverless Lakehouse Platform (AWS)
Designed a full serverless data platform using S3, Glue, Athena, and Bedrock. Built a Text-to-SQL GenAI agent with Claude 3 Sonnet for seamless querying. Resolved small file issues using Snappy and Parquet compression. Optimized ETL processes and data access for enterprise-scale performance.
**Technologies:** AWS Glue, Athena, Bedrock (GenAI), S3, PySpark

### HorizonScale — AI Capacity Forecasting Engine
Replaced legacy manual Trenda processes with a parallel generator-based pipeline, reducing forecasting cycles by 90%. Developed an interactive Streamlit dashboard for real-time capacity insights and 'High Trust' utilization scores. Built a modern agentic pipeline for banking-scale telemetry. Features comprehensive documentation, API guides, and performance benchmarks.
**Technologies:** Python, Prophet, Streamlit, PySpark, Multiprocessing, Spark, Machine Learning, Data Pipelines, API Integration, Performance Benchmarking, Testing and Validation, AI Reasoning, RAG (Retrieval-Augmented Generation)
**Repo:** [github.com/seanlgirgis/HorizonStudy](https://github.com/seanlgirgis/HorizonStudy)

### FAST Project
Focused on data mining user performance metrics to optimize critical money-generating systems. Delivered actionable insights for system enhancements.
**Technologies:** Dynatrace, Data Mining

