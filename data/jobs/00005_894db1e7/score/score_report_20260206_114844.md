# Score Report for 00005.Geico.SeniorEngineer.02062026.1146

## Match Score: 85%
## Recommendation: Strong Proceed
## Strongest Matches
- **Extensive Experience**: 20+ years in data engineering, far exceeding the job’s requirement of 4+ years in data/analytics engineering and 3+ years in software development with SQL-based technologies.
- **Technical Expertise**: Proficient in Python (15 yrs, Expert), SQL (18 yrs, Expert), and ETL design (15 yrs, Expert), aligning perfectly with core job requirements for data processing and automation.
- **Data Warehousing & Analytics**: 10+ years in data warehousing and advanced skills in capacity planning/forecasting, matching the need for dimensional modeling and data architecture (3+ years required).
- **Cloud & Big Data**: Experience with AWS serverless architectures (Glue/Athena) and big data tools like PySpark, meeting the job’s focus on cloud services (3+ years required) and big data technologies like Spark (2+ years required).
- **Leadership & Reporting**: Proven ability in executive-level reporting and mentoring, aligning with responsibilities to lead design sessions, code reviews, and provide insights on platform health.

## Gaps & Risks
- **Power BI & Power Platform Expertise**: Limited evidence of specific experience with Power BI administration, Power Platform (Dataverse, flows), or related REST APIs/Graph API, which are central to the role. *Mitigation*: Highlight transferable skills in BI tools and automation frameworks; pursue quick certification or training in Power BI and Power Platform.
- **CI/CD & DevOps Tools**: No explicit mention of experience with Azure DevOps, GitHub CI/CD pipelines, or Microsoft Fabric/Azure Data Factory, which are preferred or required. *Mitigation*: Emphasize adaptability with similar tools (e.g., AWS pipelines) and commit to rapid upskilling in Azure-specific frameworks.
- **Front-End Development**: Lacks experience in REACT/JavaScript for front-end development, noted as a plus. *Mitigation*: Not critical; focus on core strengths in back-end and data engineering during application.
- **Specific Tools**: No mention of DBT, Kafka, Airflow, or Apache Iceberg, which are either required or preferred. *Mitigation*: Showcase expertise in comparable tools (e.g., PySpark for big data) and willingness to learn.

## Advice
- Tailor your resume and cover letter to emphasize your deep expertise in Python, SQL, ETL, and cloud architectures (AWS), directly mapping these to GEICO’s needs for scalable systems and data pipelines.
- Address gaps by proactively starting online courses or certifications in Power BI (e.g., Microsoft Certified: Power BI Data Analyst) and Azure fundamentals (e.g., AZ-900) to demonstrate commitment.
- In interviews, focus on your ability to adapt and learn new tools quickly, using examples of past transitions (e.g., legacy to AWS serverless). Highlight leadership in mentoring and driving innovation.
- Network with GEICO employees on LinkedIn to understand team priorities, especially around Power BI and automation, to better align your pitch.