{
  "personal": {
    "full_name": "Sean Luka Girgis",
    "previous_name": "Emad Girgis",
    "name_change_note": "Changed approximately 9 years ago (around 2017)",
    "preferred_title": "Senior Data Engineer | Capacity Planning & AI-Driven Infrastructure Optimization",
    "address": "Murphy, TX (Plano area) / 424 Oriole Dr., Murphy, TX 75094",
    "phone": "214-315-2190",
    "email": "seanlgirgis@gmail.com",
    "linkedin": "https://www.linkedin.com/in/sean-girgis-43bb1b5/",
    "github": "https://github.com/seanlgirgis",
    "personal_website": "https://seanlgirgis.github.io",
    "x_twitter": "https://x.com/SeanLuka22249",
    "location_preference": "Plano, TX / Dallas-Fort Worth / Remote"
  },
  "summary": "Senior Data Engineer with over 20 years of enterprise experience, specializing in Machine Learning, AI, and distributed data processing systems like Apache Spark and PySpark for real-time data processing and process optimization. Proficient in Python programming, I have a proven track record of building ML pipelines for forecasting and anomaly detection. Experienced in deploying scalable data solutions and collaborating with cross-functional teams for product deployment. Passionate about driving innovation in manufacturing AI, I am eager to contribute to Oden Technologies’ mission of enhancing factory operations through advanced algorithms. My expertise in timeseries analysis and data engineering aligns with creating impactful solutions for manufacturing processes. I thrive in environments focused on customer success collaboration and delivering next-generation product features.",
  "skills": [
    "Python (15 years, Expert)",
    "Machine Learning (Advanced)",
    "PySpark (6 years, Advanced)",
    "Distributed Data Processing (Advanced)",
    "ML Workflows (Advanced)",
    "SQL / Oracle (18 years, Expert)",
    "Generative AI (2 years, Intermediate-Advanced)",
    "GCP (Advanced)",
    "Prophet / Time-Series Forecasting (5 years, Advanced)",
    "scikit-learn (6 years, Advanced)",
    "AWS (7 years, Advanced)",
    "Pandas (10 years, Expert)",
    "ETL Design & Optimization (15 years, Expert)",
    "Data Warehousing (10 years, Advanced)",
    "Streamlit (3 years, Advanced)",
    "Hive/Hadoop (7 years, Advanced)",
    "Airflow (5 years, Advanced)",
    "Capacity Planning / Forecasting (15 years, Expert)",
    "GenAI / LLM Agents (2 years, Intermediate-Advanced)",
    "C++ (20 years, Expert)",
    "Java (10 years, Advanced)",
    "Perl (12 years, Advanced)",
    "Ksh / Korn Shell Scripting (12 years, Advanced)",
    "PL/SQL (18 years, Expert)",
    "Dynatrace (AppMon + Synthetics) (8 years, Expert)",
    "CA APM / Introscope (10 years, Expert)",
    "BMC TrueSight / TSCO (7 years, Advanced)",
    "Oracle RAC (10 years, Advanced)",
    "Multiprocessing (10 years, Advanced)",
    "Docker (5 years, Advanced)",
    "Git (10 years, Advanced)",
    "Linux/Unix (20 years, Expert)",
    "OCCI / OCI (10 years, Advanced)",
    "WebLogic / WebSphere (10 years, Advanced)",
    "VB6 / VC++ (5 years, Intermediate)"
  ],
  "experience": [
    {
      "company": "CITI",
      "title": "Senior Capacity & Data Engineer",
      "start_date": "2017-11",
      "end_date": "2025-12",
      "bullets": [
        "Architected automated ETL pipelines using Python and Pandas to ingest high-scale telemetry data, enhancing real-time data processing capabilities.",
        "Developed ML forecasting models with Prophet and scikit-learn for capacity bottleneck prediction, improving provisioning accuracy by anticipating issues 6 months ahead.",
        "Designed optimized Oracle schemas for historical data retention, supporting timeseries analysis and seasonal risk forecasting.",
        "Integrated disparate data feeds into unified reporting systems with executive dashboards, enabling process optimization and anomaly detection.",
        "Applied statistical analysis and machine learning to identify underutilized infrastructure, driving significant cost savings through hardware consolidation.",
        "Automated reporting workflows with Python scripts, streamlining data preparation and monitoring processes for enterprise-scale systems."
      ]
    },
    {
      "company": "G6 Hospitality LLC",
      "title": "Performance Engineer",
      "start_date": "2017-03",
      "end_date": "2017-11",
      "bullets": [
        "Managed Dynatrace AppMon/Synthetics for critical systems, focusing on real-time performance monitoring and alerting.",
        "Led 'FAST' project to data-mine user performance metrics, providing optimization recommendations for key revenue-generating systems.",
        "Upgraded Dynatrace infrastructure (6.5 to 7.0) and supported cloud migration to AWS, ensuring robust end-to-end monitoring.",
        "Analyzed system bottlenecks and suggested performance improvements, contributing to process optimization.",
        "Provided detailed dashboarding for before/after metrics, supporting data-driven decision-making.",
        "Integrated Performance Center with Dynatrace for comprehensive performance insights."
      ]
    },
    {
      "company": "HCL / Entergy",
      "title": "APM Consultant",
      "start_date": "2017-01",
      "end_date": "2017-03",
      "bullets": [
        "Supported enterprise CA APM, CEM, and ADA for utility systems, ensuring reliable performance monitoring.",
        "Managed and supported monitoring solutions to maintain system stability and uptime."
      ],
      "exclude_from_resume": true
    },
    {
      "company": "CA Technologies (various clients) & Enterprise Iron (TIAA-CREF)",
      "title": "Senior Consultant / SME for CA APM",
      "start_date": "2011",
      "end_date": "2016",
      "bullets": [
        "Led large-scale CA APM implementations and upgrades (9.1 to 10.1), managing 4,000–6,000 agents for enterprise systems.",
        "Designed custom Management Modules, dashboards, and alerts to support real-time monitoring and anomaly detection.",
        "Developed Perl/Ksh scripts for data extraction, automating reporting and data preparation processes.",
        "Provided sizing recommendations and bottleneck resolution for J2EE/.NET environments, enhancing system performance.",
        "Trained client teams on APM solutions, fostering effective customer success collaboration.",
        "Collaborated with IT teams to troubleshoot performance issues, ensuring optimal system deployment."
      ]
    },
    {
      "company": "AT&T",
      "title": "Performance Test Engineer",
      "start_date": "2010-08",
      "end_date": "2011-07",
      "bullets": [
        "Analyzed J2EE telecom applications to identify load and resource bottlenecks, focusing on performance optimization.",
        "Documented critical metrics including JDBC connections, threads, memory, CPU, and garbage collection for system monitoring.",
        "Installed JMX, Thread Dumps, and Wily Introscope to enhance real-time data processing and monitoring.",
        "Created automation scripts to streamline performance testing and reporting workflows."
      ]
    },
    {
      "company": "Sabre",
      "title": "Senior Systems & Data Migration Engineer",
      "start_date": "2008-05",
      "end_date": "2012",
      "bullets": [
        "Led massive data migration of a high-throughput shopping engine from 200+ MySQL nodes to a 6-node Oracle RAC cluster.",
        "Optimized core transaction processing using C++ and OCCI, reducing hardware footprint by 95% while maintaining sub-second latency.",
        "Built a CPPUNIT testing framework to automate conversion and validation processes.",
        "Ensured high-performance data processing for a system handling 10x the throughput of VISA.",
        "Collaborated with engineering teams for seamless deployment of optimized solutions.",
        "Focused on process optimization to achieve significant infrastructure efficiency gains."
      ]
    },
    {
      "company": "Computer Science Corporation (CSC)",
      "title": "Architect/Developer (IRS CADE Project)",
      "start_date": "2007-10",
      "end_date": "2008-05",
      "bullets": [
        "Performed UML-based unit design for a CICS/MQSeries/XML/DB2 system as part of IRS modernization.",
        "Developed modules to support critical data processing and system integration.",
        "Worked on messaging architecture using VC++ and DB2 for robust data handling."
      ]
    },
    {
      "company": "Corpus Inc. / Sprint (AMDOCS projects)",
      "title": "Developer / Support Engineer (Billing, Interfaces, CSM/PRMS)",
      "start_date": "2001",
      "end_date": "2007",
      "bullets": [
        "Developed high-availability multithreaded C++ interfaces using POSIX, sockets, and Marconi APIs for billing systems.",
        "Achieved performance tuning in billing processes (C/C++/Pro*C/PL/SQL), reducing memory usage by 75% and improving throughput by 20%.",
        "Automated system administration tasks for WebLogic/WebSphere using Korn Shell scripts, enhancing operational efficiency.",
        "Designed and implemented database fixes, improving performance by 10x through optimized sequences.",
        "Troubleshot and supported Enabler/CSM/EMS modules, ensuring system reliability.",
        "Collaborated with vendors and teams for impact assessments and solution deployment."
      ]
    },
    {
      "company": "Simplex International - Canada",
      "title": "Developer",
      "start_date": "1999",
      "end_date": "2001",
      "bullets": [
        "Developed time and attendance interfaces using VB6/VC++ to support business operations."
      ]
    },
    {
      "company": "Humber College",
      "title": "Lab Support",
      "start_date": "1996-09",
      "end_date": "1999-04",
      "bullets": [
        "Maintained and troubleshot Linux/Windows systems, ensuring reliable lab operations.",
        "Performed various administrative tasks to support IT infrastructure and user needs."
      ]
    }
  ],
  "projects": [
    {
      "name": "Serverless Lakehouse Platform (AWS)",
      "description": "Designed a full serverless data platform using S3, Glue, Athena, and Bedrock. Built a Text-to-SQL GenAI agent with Claude 3 Sonnet for enhanced querying. Resolved small file issues with Snappy and Parquet compression. Optimized ETL processes and data access for enterprise-scale operations.",
      "technologies": [
        "AWS Glue",
        "Athena",
        "Bedrock (GenAI)",
        "S3",
        "PySpark"
      ],
      "timeframe": "Recent"
    },
    {
      "name": "HorizonScale — AI Capacity Forecasting Engine",
      "description": "Replaced legacy manual processes with a parallel generator-based pipeline, reducing forecasting cycles by 90%. Developed an interactive Streamlit dashboard for real-time capacity insights and 'High Trust' utilization scores. Created a modern agentic pipeline for banking-scale telemetry. Features comprehensive documentation, API guides, and performance benchmarks.",
      "technologies": [
        "Python",
        "Prophet",
        "Streamlit",
        "PySpark",
        "Multiprocessing",
        "Spark",
        "Machine Learning",
        "Data Pipelines",
        "API Integration",
        "Performance Benchmarking",
        "Testing and Validation",
        "AI Reasoning",
        "RAG (Retrieval-Augmented Generation)"
      ],
      "repo": "https://github.com/seanlgirgis/HorizonStudy",
      "documentation": [
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/README.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/API_AND_INTEGRATION_GUIDE.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/FUTURE_ROADMAP.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/OPERATIONAL_RUNBOOK.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/PERFORMANCE_BENCHMARK_REPORT.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/TESTING_AND_VALIDATION_STRATEGY.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/TREND_TO_HORIZONSCALE_EVOLUTION.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/lessons_learnt.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/AI_REASONING_AND_RAG_SPEC.md"
      ],
      "code": "https://github.com/seanlgirgis/HorizonStudy/tree/main/src/HorizonScale",
      "additional_docs": [
        "https://github.com/seanlgirgis/HorizonStudy/tree/main/Docs/lib",
        "https://github.com/seanlgirgis/HorizonStudy/tree/main/Docs/pipeline",
        "https://github.com/seanlgirgis/HorizonStudy/tree/main/Docs/synthetic"
      ],
      "timeframe": "Recent"
    },
    {
      "name": "FAST Project",
      "description": "Conducted data mining on user performance metrics to optimize critical money-generating systems. Focused on identifying bottlenecks and enhancing system efficiency.",
      "technologies": [
        "Dynatrace",
        "Data Mining"
      ],
      "timeframe": "2017"
    }
  ],
  "education": [
    {
      "degree": "High Diploma / Post-Graduate Diploma in Computer Engineering Technology / Computer Science",
      "institution": "Humber College",
      "location": "Toronto, Canada",
      "dates": "Not specified"
    },
    {
      "degree": "Bachelor of Science in Civil Engineering",
      "institution": "Zagazig University",
      "location": "Egypt / Cairo, Egypt",
      "dates": "Not specified"
    }
  ]
}