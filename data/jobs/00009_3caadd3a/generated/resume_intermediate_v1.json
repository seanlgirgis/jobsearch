{
  "personal": {
    "full_name": "Sean Luka Girgis",
    "previous_name": "Emad Girgis",
    "name_change_note": "Changed approximately 9 years ago (around 2017)",
    "preferred_title": "Senior Data Engineer | Capacity Planning & AI-Driven Infrastructure Optimization",
    "address": "Murphy, TX (Plano area) / 424 Oriole Dr., Murphy, TX 75094",
    "phone": "214-315-2190",
    "email": "seanlgirgis@gmail.com",
    "linkedin": "https://www.linkedin.com/in/sean-girgis-43bb1b5/",
    "github": "https://github.com/seanlgirgis",
    "personal_website": "https://seanlgirgis.github.io",
    "x_twitter": "https://x.com/SeanLuka22249",
    "location_preference": "Plano, TX / Dallas-Fort Worth / Remote"
  },
  "summary": "Senior Data Engineer with over 20 years of enterprise experience, specializing in designing and maintaining robust data pipelines and ETL processes. Proficient in SQL, data warehousing, and data modeling, with hands-on expertise in Snowflake and AWS serverless architectures. Skilled in optimizing data quality and creating impactful data visualizations for analytics and reporting. Adept at collaborating with cross-functional teams to deliver scalable data solutions. Passionate about contributing to RTX's mission of advancing aerospace and defense through innovative data engineering at Collins Aerospace. Eager to apply my skills in data pipelines and ETL processes to support a hybrid role within a dynamic, fast-paced environment.",
  "skills": [
    "SQL / Oracle (18 years, Expert, Last Used: 2025)",
    "ETL Design & Optimization (15 years, Expert, Last Used: 2025)",
    "Data Warehousing (10 years, Advanced, Last Used: 2025)",
    "Python (15 years, Expert, Last Used: 2025)",
    "Pandas (10 years, Expert, Last Used: 2025)",
    "AWS (7 years, Advanced, Last Used: 2025)",
    "PySpark (6 years, Advanced, Last Used: 2025)",
    "Capacity Planning / Forecasting (15 years, Expert, Last Used: 2025)",
    "Prophet / Time-Series Forecasting (5 years, Advanced, Last Used: 2025)",
    "scikit-learn (6 years, Advanced, Last Used: 2025)",
    "GenAI / LLM Agents (2 years, Intermediate-Advanced, Last Used: 2025)",
    "Streamlit (3 years, Advanced, Last Used: 2025)",
    "Hive/Hadoop (7 years, Advanced, Last Used: 2025)",
    "Airflow (5 years, Advanced, Last Used: 2025)",
    "Docker (5 years, Advanced, Last Used: 2025)",
    "Git (10 years, Advanced, Last Used: 2025)",
    "Linux/Unix (20 years, Expert, Last Used: 2025)",
    "Multiprocessing (10 years, Advanced, Last Used: 2025)",
    "BMC TrueSight / TSCO (7 years, Advanced, Last Used: 2025)",
    "C++ (20 years, Expert, Last Used: 2010)",
    "Java (10 years, Advanced, Last Used: 2016)",
    "Perl (12 years, Advanced, Last Used: 2016)",
    "Ksh / Korn Shell Scripting (12 years, Advanced, Last Used: 2016)",
    "PL/SQL (18 years, Expert, Last Used: 2010)",
    "Oracle RAC (10 years, Advanced, Last Used: 2010)",
    "OCCI / OCI (10 years, Advanced, Last Used: 2010)",
    "WebLogic / WebSphere (10 years, Advanced, Last Used: 2011)",
    "Dynatrace (AppMon + Synthetics) (8 years, Expert, Last Used: 2017)",
    "CA APM / Introscope (10 years, Expert, Last Used: 2017)",
    "VB6 / VC++ (5 years, Intermediate, Last Used: 2001)"
  ],
  "experience": [
    {
      "company": "CITI",
      "title": "Senior Capacity & Data Engineer",
      "start_date": "2017-11",
      "end_date": "2025-12",
      "bullets": [
        "Architected automated ETL pipelines using Python and Pandas to ingest telemetry data from 6,000+ endpoints, enhancing data quality and reporting efficiency.",
        "Designed optimized Oracle schemas for historical data retention, supporting advanced data modeling and seasonal risk forecasting.",
        "Developed machine learning forecasting models with Prophet and scikit-learn to predict capacity bottlenecks six months ahead, improving provisioning accuracy.",
        "Built unified reporting solutions by integrating disparate data feeds into Oracle, creating executive dashboards for actionable insights.",
        "Identified underutilized infrastructure through data mining, driving significant cost savings via hardware consolidation.",
        "Automated data cleansing and validation processes to ensure high data integrity across banking infrastructure."
      ]
    },
    {
      "company": "G6 Hospitality LLC",
      "title": "Performance Engineer",
      "start_date": "2017-03",
      "end_date": "2017-11",
      "bullets": [
        "Managed Dynatrace AppMon/Synthetics for critical systems, ensuring robust performance monitoring for Brand.com.",
        "Led 'FAST' project to data-mine real-user performance metrics, providing optimization recommendations for key systems.",
        "Upgraded Dynatrace from 6.5 to 7.0, implemented TLS1.2 security, and supported cloud migration to AWS.",
        "Developed dashboards for end-to-end functionality, delivering before/after metrics for performance analysis.",
        "Analyzed bottlenecks and proposed actionable performance improvements to enhance system efficiency."
      ]
    },
    {
      "company": "HCL / Entergy",
      "title": "APM Consultant",
      "start_date": "2017-01",
      "end_date": "2017-03",
      "bullets": [
        "Supported enterprise CA APM, CEM, and ADA for utility systems, ensuring reliable monitoring.",
        "Managed and supported monitoring solutions (CA APM, CA CEM, CA ADA) for critical infrastructure."
      ],
      "exclude_from_resume": true
    },
    {
      "company": "CA Technologies (various clients) & Enterprise Iron (TIAA-CREF)",
      "title": "Senior Consultant / SME for CA APM",
      "start_date": "2011",
      "end_date": "2016",
      "bullets": [
        "Led large-scale CA APM implementations and upgrades (9.1 to 10.1), managing 4,000–6,000 agents for enterprise clients.",
        "Designed custom Management Modules, dashboards, and alerts, along with Perl/Ksh scripts for data extraction.",
        "Provided sizing recommendations, created Golden Images, and delivered client training on APM solutions.",
        "Collaborated with IT teams to troubleshoot and resolve performance bottlenecks in J2EE and .NET environments.",
        "Optimized agent and enterprise manager installations to ensure scalability and performance."
      ]
    },
    {
      "company": "AT&T",
      "title": "Performance Test Engineer",
      "start_date": "2010-08",
      "end_date": "2011-07",
      "bullets": [
        "Analyzed J2EE telecom applications to identify load and break points, focusing on performance optimization.",
        "Documented critical metrics including JDBC connections, threads, memory, CPU, and garbage collection.",
        "Installed JMX, Thread Dumps, and Wily Introscope for monitoring and performance analysis.",
        "Created automation scripts to streamline performance testing and data collection processes."
      ]
    },
    {
      "company": "Sabre",
      "title": "Senior Systems & Data Migration Engineer",
      "start_date": "2008-05",
      "end_date": "2012",
      "bullets": [
        "Led large-scale migration of 200+ MySQL nodes to a 6-node Oracle RAC cluster for a high-throughput shopping engine.",
        "Optimized C++/OCCI transaction processing, reducing hardware footprint by 95% while maintaining sub-second latency.",
        "Built a CPPUNIT testing framework to automate conversion and ensure data integrity.",
        "Collaborated with teams to ensure seamless data migration and system performance under high load."
      ]
    },
    {
      "company": "Computer Science Corporation (CSC)",
      "title": "Architect/Developer (IRS CADE Project)",
      "start_date": "2007-10",
      "end_date": "2008-05",
      "bullets": [
        "Performed UML-based unit design for a CICS/MQSeries/XML/DB2 system as part of IRS modernization.",
        "Developed modules to support critical system functionality and data processing.",
        "Worked on messaging architecture using VC++ and DB2 for robust integration."
      ]
    },
    {
      "company": "Corpus Inc. / Sprint (AMDOCS projects)",
      "title": "Developer / Support Engineer (Billing, Interfaces, CSM/PRMS)",
      "start_date": "2001",
      "end_date": "2007",
      "bullets": [
        "Developed high-availability multithreaded C++ interfaces using POSIX, sockets, and Marconi APIs for billing systems.",
        "Enhanced billing performance with C/C++/Pro*C/PL/SQL, achieving 75% memory reduction and 10x database performance.",
        "Automated system administration for WebLogic/WebSphere environments using Korn Shell scripts.",
        "Designed and implemented interfaces with UML in Rational Rose, incorporating design patterns for scalability.",
        "Troubleshot and supported Enabler/CSM/EMS modules, ensuring operational stability."
      ]
    },
    {
      "company": "Simplex International - Canada",
      "title": "Developer",
      "start_date": "1999",
      "end_date": "2001",
      "bullets": [
        "Developed time and attendance interfaces using VB6 and VC++ for operational efficiency."
      ]
    },
    {
      "company": "Humber College",
      "title": "Lab Support",
      "start_date": "1996-09",
      "end_date": "1999-04",
      "bullets": [
        "Maintained and troubleshot Linux and Windows systems as part of the Lab Support team.",
        "Performed various administrative tasks to ensure smooth lab operations."
      ]
    }
  ],
  "education": [
    {
      "degree": "High Diploma / Post-Graduate Diploma in Computer Engineering Technology / Computer Science",
      "institution": "Humber College",
      "location": "Toronto, Canada",
      "dates": "Not specified"
    },
    {
      "degree": "Bachelor of Science in Civil Engineering",
      "institution": "Zagazig University",
      "location": "Egypt / Cairo, Egypt",
      "dates": "Not specified"
    }
  ],
  "projects": [
    {
      "name": "Serverless Lakehouse Platform (AWS)",
      "description": "Designed a full serverless data platform using S3, Glue, Athena, and Bedrock. Built a Text-to-SQL GenAI agent with Claude 3 Sonnet for enhanced querying. Resolved small file issues with Snappy and Parquet compression. Optimized ETL processes and data access for enterprise-scale operations.",
      "technologies": [
        "AWS Glue",
        "Athena",
        "Bedrock (GenAI)",
        "S3",
        "PySpark"
      ],
      "timeframe": "Recent"
    },
    {
      "name": "HorizonScale — AI Capacity Forecasting Engine",
      "description": "Replaced legacy manual Trenda processes with a parallel generator-based pipeline, reducing forecasting cycles by 90%. Developed an interactive Streamlit dashboard for real-time capacity insights and 'High Trust' utilization scores. Created a modern agentic pipeline for banking-scale telemetry. Repository includes code, documentation, and performance benchmarks.",
      "technologies": [
        "Python",
        "Prophet",
        "Streamlit",
        "PySpark",
        "Multiprocessing",
        "Spark",
        "Machine Learning",
        "Data Pipelines",
        "API Integration",
        "Performance Benchmarking",
        "Testing and Validation",
        "AI Reasoning",
        "RAG (Retrieval-Augmented Generation)"
      ],
      "repo": "https://github.com/seanlgirgis/HorizonStudy",
      "documentation": [
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/README.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/API_AND_INTEGRATION_GUIDE.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/FUTURE_ROADMAP.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/OPERATIONAL_RUNBOOK.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/PERFORMANCE_BENCHMARK_REPORT.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/TESTING_AND_VALIDATION_STRATEGY.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/TREND_TO_HORIZONSCALE_EVOLUTION.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/lessons_learnt.md",
        "https://github.com/seanlgirgis/HorizonStudy/blob/main/Docs/AI_REASONING_AND_RAG_SPEC.md"
      ],
      "code": "https://github.com/seanlgirgis/HorizonStudy/tree/main/src/HorizonScale",
      "additional_docs": [
        "https://github.com/seanlgirgis/HorizonStudy/tree/main/Docs/lib",
        "https://github.com/seanlgirgis/HorizonStudy/tree/main/Docs/pipeline",
        "https://github.com/seanlgirgis/HorizonStudy/tree/main/Docs/synthetic"
      ],
      "timeframe": "Recent"
    },
    {
      "name": "FAST Project",
      "description": "Conducted data mining on user performance metrics to optimize critical money-generating systems. Provided actionable insights for system enhancements.",
      "technologies": [
        "Dynatrace",
        "Data Mining"
      ],
      "timeframe": "2017"
    }
  ]
}