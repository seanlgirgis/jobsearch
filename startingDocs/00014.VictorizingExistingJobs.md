### Strategy for Vectorizing Existing and New Job Data

First, acknowledging your setup: You've got 16 processed jobs in `data/jobs/`, each with raw descriptions (e.g., `raw/job_description.md` or original intake .md files), metadata.yaml, and other artifacts. The goal is to vectorize these for RAG-based features like duplicate checking (via FAISS + all-MiniLM-L6-v2 embeddings, as per the Constitution). This aligns with evolving toward a "second brain" with searchable job knowledge graph.

The strategy should prioritize:
- **Modularity**: Keep vectorization logic decoupled (e.g., in a utility script) to avoid bloating core pipeline scripts.
- **Idempotency**: Re-running should be safe and efficient; prefer incremental updates over full rebuilds long-term.
- **Automation**: Integrate seamlessly into the v0 pipeline without disrupting existing flows (01-10).
- **Scalability**: With 16 now, but potentially 100+, focus on fast queries (<100ms) and reasonable build times (~seconds for embeddings).
- **Ethical/Practical**: Only index public/non-sensitive data (job descriptions, metadata excerpts); no full resumes or personal notes yet.

#### Step 1: Vectorizing the Existing 16 Jobs (One-Time Bootstrap)
- **Approach**: Use a dedicated utility script (e.g., `scripts/utils/build_job_index.py`) to scan all `data/jobs/*` folders, extract/embed job descriptions, and build the FAISS index + YAML metadata cache.
  - Why separate? Follows Core Principles (modular, single responsibility). Can be run manually now, then automated.
  - What to vectorize: Primarily the raw job description text (cleaned/normalized: remove headers/footers if noisy, lowercase optional). Optionally include key metadata fields (company, role) as concatenated text for richer semantic search.
  - Output: `data/job_index/faiss_job_descriptions.index` (binary vector store) and `data/job_index/jobs_metadata.yaml` (for quick UUID/company/role/apply_date lookups without reloading full yamls).
- **Process**:
  1. Collect: Iterate over all 16+ folders, load description text (prioritize `raw/job_description.md`, fallback to any *.md).
  2. Embed: Batch-encode with sentence-transformers (local, no API calls needed).
  3. Index: Add to FAISS IndexFlatIP (cosine similarity).
  4. Store metadata: List of dicts with UUID, path, company, etc.
- **Edge Cases**: Skip folders without valid .md; handle large texts by chunking (e.g., split into 512-token segments if >2000 words).
- **Execution**: Run once manually with `--rebuild` flag. Time estimate: <5 seconds for 16 docs on a standard machine.
- **Benefits**: Immediate usability for duplicate checks; sets up for broader RAG (e.g., querying past jobs for patterns).

#### Step 2: Integrating Vectorization for New Entries (Pipeline Extension)
- **High-Level Design**: Add a post-processing hook to automatically update the index after a new job is fully ingested and accepted. This ensures the data store is always current without manual intervention.
  - **Where to Hook?** Comparison table for options (01, 02, 03, or end of 10):

    | Script/Step | Pros | Cons | Recommendation |
    |-------------|------|------|----------------|
    | **01_score_job.py** (post-intake, after creating job folder) | Early addition; catches all jobs (even rejected). | Indexes low-quality/rejected jobs; wastes resources if not accepted. | Low priority – too early if we want only "viable" jobs. |
    | **02_decide_job.py** (post-acceptance) | Only indexes accepted jobs; aligns with "proceed to tailor" intent. | Misses if decision is manual/outside pipeline; requires flag check (e.g., if --accept). | Strong candidate – acceptance signals "worth storing long-term." |
    | **03_tailor_job_data.py** (post-tailoring) | Has structured YAML ready; could embed requirements/skills for better search. | Slightly later; assumes acceptance happened. | Viable if we want enriched embeddings (description + tailored data). |
    | **End of 10_auto_pipeline.py** (after full run) | Covers entire automated flow; only on success. | Doesn't catch manual script runs; full pipeline might fail mid-way. | Best for v0 – simple, comprehensive for main usage. |

    **Recommended Placement**: End of `10_auto_pipeline.py` (after step 09). 
    - Reason: It's the primary entry point (per rules.md). This keeps changes minimal – just add a subprocess call to the utility.
    - Alternative: If you prefer per-step granularity, hook into 02 (on --accept) for selectivity, then call from 10 as well.
- **Update Mechanism**:
  - **v0 Simple**: Full rebuild each time (fast for <100 jobs; ~1-2s extra per pipeline run).
  - **v1 Incremental**: Check if job UUID already in metadata; if new, embed and add to FAISS (use IndexIDMap for IDs). Rebuild only on major changes (e.g., model swap).
  - Flags: Utility supports `--incremental --uuid <new_uuid>` for targeted adds.
- **Workflow Impact**:
  - No disruption to existing scripts: Utility is external.
  - Error Handling: If indexing fails (rare), log warning but don't halt pipeline (fail gracefully).
  - Future Expansion: Once indexed, enable RAG queries (e.g., "find jobs with LLM experience" via semantic search in a new script).

#### Overall Design Considerations
- **Data Store Structure**: Keep `data/job_index/` as a dedicated folder (gitignore the .index binary if large; commit metadata.yaml for traceability).
- **Standards Alignment**: Uses pathlib, argparse; enforces UTF-8. Follow CodingStyle.md (type hints, docstrings).
- **Testing**: Manual: Run utility, then query with 00_check_applied_before.py on a known duplicate. Automated: Add unit tests in v1 for embedding consistency.
- **Risks/Mitigations**:
  - Staleness: Cron/schedule periodic rebuilds if pipeline isn't run often.
  - Privacy: Only embed public job data; exclude any personal annotations.
  - Cost: Embeddings are local/offline – zero API hits.
- **Next Evolution**: Once in place, extend to vectorize more (e.g., cover letters, research YAML) for full "second brain" search. Track in user story 100.003: "As a user, I want automatic indexing of new jobs so the RAG store stays up-to-date."

This strategy keeps things iterative/MVP-focused. Thoughts on the hook placement – prefer 02 for acceptance gating, or stick with 10 for simplicity? Once finalized, we can spec the code changes.